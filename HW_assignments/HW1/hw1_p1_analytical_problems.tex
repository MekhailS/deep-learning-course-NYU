\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}


\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}

% \usepackage[margin=1in]{geometry} 
\usepackage{bm}
\newcommand{\matr}[1]{\bm{#1}}     % ISO complying version
\newcommand{\vect}[1]{\bm{#1}}     % ISO complying version
\DeclareMathOperator{\softmax}{softmax}


\usepackage[hyphens]{url}
\usepackage[colorlinks]{hyperref}
\usepackage{enumitem}

\makeatletter
\newcommand\RedeclareMathOperator{%
  \@ifstar{\def\rmo@s{m}\rmo@redeclare}{\def\rmo@s{o}\rmo@redeclare}%
}
% this is taken from \renew@command
\newcommand\rmo@redeclare[2]{%
  \begingroup \escapechar\m@ne\xdef\@gtempa{{\string#1}}\endgroup
  \expandafter\@ifundefined\@gtempa
     {\@latex@error{\noexpand#1undefined}\@ehc}%
     \relax
  \expandafter\rmo@declmathop\rmo@s{#1}{#2}}
% This is just \@declmathop without \@ifdefinable
\newcommand\rmo@declmathop[3]{%
  \DeclareRobustCommand{#2}{\qopname\newmcodes@#1{#3}}%
}
\@onlypreamble\RedeclareMathOperator
\makeatother

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\V}{\mathbb{V}}
\RedeclareMathOperator{\Pr}{\mathbb{P}}


\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{DS-GA 1008: Deep Learning, Spring 2020}
\newcommand\hwnumber{1}                  % <-- homework number

\pagestyle{fancyplain}
\headheight 55pt
%\lhead{\NetIDa}
% \lhead{\NetIDa\\\NetIDb}                 % <-- Comment this line out for problem sets (make sure you are person #1)
\chead{\textbf{\Large \course \\ Homework Assignment \hwnumber}\\ }
%\rhead{\course \\ \today}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}
\begin{center}
\texttt{He who learns but does not think is lost. \\He who thinks but does not learn is in great danger. \\ Confucius (551 - 479 BC)}

\end{center}

\section*{1. Backprop}

Backpropagation or ``backward propagation through errors'' is a method which calculates the gradient of the loss function of a neural network with respect to its weights. 

\subsection*{1.1. Affine Module}
The chain rule is at the heart of backpropagation. Suppose we are given $\vect{x} \in \mathbb{R}^2$ and that we use an affine transformation with parameters
$\matr{W} \in \mathbb{R}^{2\times 2}$ and $\vect{b}\in \mathbb{R}^2$ defined by:
\begin{align}
\vect{y} = \matr{W} \vect{x} + \vect{b},
\end{align}
\begin{enumerate}[(a)]
    \item Suppose an arbitrary cost function $C(\vect{y})$ and that we are given the gradient $\partial C / \partial \vect{y}$. Give an expression for $ \partial C / \partial \matr{W}$ and $ \partial C / \partial \vect{b}$ in terms of $ \partial C / \partial \vect{y}$ and $\vect{x}$ using the chain rule.
    
    \textbf{Answer}:
    \begin{align}
    \frac{\partial C}{\partial W} = \frac{\partial C}{\partial y}\frac{\partial y}{\partial W} = \frac{\partial C}{\partial y}x \\
    \frac{\partial C}{\partial b} = \frac{\partial C}{\partial y}\frac{\partial y}{\partial b} = \frac{\partial C}{\partial y}
    \end{align}
    
    \item If we define a new cost $C_2(\vect{y}) = 3*C(\vect{y})$, do we know how our gradients with respect to $\vect{W,b}$ change without knowing the particular form of $C(\vect{y})$ or $ \partial C / \partial \vect{y}$?
    
    \textbf{Answer}:
    \begin{align}
    \frac{\partial C_2}{\partial W} = 3*\frac{\partial C}{\partial W}\\
    \frac{\partial C_2}{\partial b} = 3*\frac{\partial C}{\partial b}
    \end{align}
    
    
\end{enumerate}



\subsection*{1.2. Softmax Module}
The softmax expression is at the crux of multi-class classification.
After receiving $K$ unconstrained values in the form of a vector $\vect{x} \in \mathbb{R}^K$, the softmax function normalizes these values to $K$ positive values that all sum to $1$. The softmax is defined as
\begin{align}
\vect{y}=\softmax_\beta(\vect{x}),
\quad 
 y_k = \frac{\exp(\beta x_k)}{\sum_{n} \exp(\beta x_{n})},
\end{align}
where $\sum_y^K y_k = 1$, $y_k \geq 0$ for all $k$, and $\exp(z)=e^z$. We usually let the softmax input $\vect{x} \in \mathbb{R}^K$ be the output of a preceding module (some feature representation) whose input is a datapoint $d$. Then we interpret $y_k$ as the probability that datapoint $d$ belongs to class $k$.\\

\noindent Since this module can be connected to others in backprop using the chain rule, we just need to compute the softmax's gradient in isolation. What is the expression for $\partial y_i / \partial x_j$?
(Hint: Answer differs when $i = j$ and $i \neq j$).

\textbf{Answer}:
\begin{align}
i \neq j: \frac{\partial y_i}{\partial x_j} = -\frac{\exp(\beta x_i)}{(\sum_{n} \exp(\beta x_{n}))^2}
\beta\exp(\beta x_j)\\
i = j: \frac{\partial y_i}{\partial x_i} = \frac{\sum_{n \neq i} \exp(\beta x_n)}{(\sum_{n} \exp(\beta x_{n}))^2}\beta\exp(\beta x_i)\\
(\frac{\exp(\beta x_i)}{\sum_{n} \exp(\beta x_{n})} = 1 - \frac{\sum_{n \neq i} \exp(\beta x_n)}
{\sum_{n} \exp(\beta x_n)})
\end{align}

\newpage
\end{document}